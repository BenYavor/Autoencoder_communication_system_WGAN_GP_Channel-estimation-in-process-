{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Communication Autoencoder with WGAN-PL alternating Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenYavor/Autoencoder_communication_system_WGAN_GP_Channel-estimation-in-process-/blob/master/Communication_Autoencoder_with_WGAN_PL_alternating_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3Mvn1V30ejH",
        "colab_type": "code",
        "outputId": "c25695f1-dce6-4a9a-da82-977eab299a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt   \n",
        "import warnings\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
        "    import tensorflow as tf\n",
        "import os\n",
        "tf.__version__\n",
        "from tensorflow import keras\n",
        "import time\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import pandas as pd\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "from scipy import special\n",
        "from tensorflow.keras import layers\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.17.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (2.0.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (2.0.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (42.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.7)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wlZswcMF7Rt",
        "colab_type": "text"
      },
      "source": [
        "#### Vergleich\n",
        "Als erstes für feste $k$ und $n$, was sich ändert ist die Samplesize, Anzahl der Samples und SNR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qpY-gawAf-9",
        "colab_type": "text"
      },
      "source": [
        "###Systemparameter\n",
        "ACHTUNG: CHANNELANZAHL WURDE UNTERSCHIEDLICH VERWENDET \\\\\n",
        "$k$ - die Anzhal der bits \\\\\n",
        "$M$ - Anzahl der unterschiedlichen Nachrichten \\\\\n",
        "$n$ - channel uses\\\\\n",
        "$N$ - Länge des Rauschvektors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "czeNNfpY1qc2",
        "outputId": "df84e00f-3264-4c4d-d97c-339ef3139303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "k = 4      # Number of information bits per message, i.e., M=2**k\n",
        "M = 2**k\n",
        "n = 2    # Number of real channel uses per message\n",
        "#k = int(np.log2(M))\n",
        "#n = 2\n",
        "print(M)\n",
        "\n",
        "SNR = 7\n",
        "time_to_train_w_gan = 0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA4TqJBOXXIg",
        "colab_type": "text"
      },
      "source": [
        "## Training Parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJZ_cnp9V-Q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_learning_rate=0.0001 \n",
        "disc_learning_rate = 0.0001                  # 0.0001  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb-DiBwSN255",
        "colab_type": "text"
      },
      "source": [
        "### Different Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFMMLrY0LthL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randN_initial = keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
        "ones_initial = keras.initializers.GlorotNormal()\n",
        "\n",
        "EncIn = tf.keras.layers.Input(shape=(M,))#, dtype= tf.int32)\n",
        "e1 = tf.keras.layers.Dense(2*n, activation=None)\n",
        "e2 = tf.keras.layers.Lambda(lambda x:tf.reshape(x, shape=[-1,int(n/2),2]))\n",
        "EncOut = tf.keras.layers.Lambda(lambda x: x/tf.sqrt(2*tf.reduce_mean(tf.square(x))))\n",
        "GenIn = tf.keras.layers.Lambda(lambda x:tf.reshape(x,(tf.shape(x)[0],-1)))\n",
        "# = tf.keras.layers.Lambda(generator)\n",
        "DecIn = tf.keras.layers.Lambda(lambda x:tf.reshape(x, shape=[-1,int(n/2),2]))\n",
        "d1 = tf.keras.layers.Lambda(lambda x:tf.reshape(x, shape=[-1,n]))\n",
        "d2 = tf.keras.layers.Dense(M, activation='relu')\n",
        "DecOut = tf.keras.layers.Dense(M, activation='softmax')\n",
        "\n",
        "\n",
        "#noise_std = EbNo_to_noise(TRAINING_SNR)\n",
        "# custom functions / layers without weights\n",
        "norm_layer = keras.layers.Lambda(lambda x: tf.divide(x,tf.sqrt(2*tf.reduce_mean(tf.square(x)))))\n",
        "shape_layer = keras.layers.Lambda(lambda x: tf.reshape(x, shape=[-1,2,n]))\n",
        "shape_layer2 = keras.layers.Lambda(lambda x: tf.reshape(x, shape=[-1,n]))\n",
        "channel_layer = keras.layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=noise_std))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J96hJhKO9VJ",
        "colab_type": "text"
      },
      "source": [
        "### Help functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV7pjryDv4M4",
        "colab_type": "code",
        "outputId": "802763ce-8b69-475c-e627-e6b8e000effd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "def EbNo2Sigma(ebnodb):\n",
        "    '''Convert Eb/No in dB to noise standard deviation'''\n",
        "    ebno = 10**(ebnodb/10)\n",
        "    return 1/np.sqrt(2*(2*k/n)*ebno)\n",
        "\n",
        "def EbNo_to_noise(ebnodb):\n",
        "    '''Transform EbNo[dB]/snr to noise power'''\n",
        "    ebno = 10**(ebnodb/10)\n",
        "    noise_std = 1/np.sqrt(2*(2*k/n)*ebno) \n",
        "    return noise_std\n",
        "\n",
        "\n",
        "def real_channel(x,noise_std):\n",
        "    # Black-box Channel\n",
        "    #AWGN\n",
        "    return x + tf.random.normal(tf.shape(x), mean=0.0, stddev=noise_std)\n",
        "\n",
        "def rayleigh_channel(x,noise_std):\n",
        "    return x + tf.sqrt(tf.square(tf.random.normal(tf.shape(x), mean=0.0, stddev=noise_std)) + tf.square(tf.random.normal(tf.shape(x), mean=0.0, stddev=noise_std)))\n",
        "    \n",
        "    #Uniform U(-3;3)    \n",
        "    #return x + tf.random_uniform(tf.shape(x), minval=-2, maxval=2)\n",
        "\n",
        "def B_Ber(input_msg, msg):\n",
        "    '''Calculate the Batch Bit Error Rate'''\n",
        "    pred_error = tf.not_equal(tf.argmax(msg, 1), tf.argmax(input_msg, 1))\n",
        "    bber = tf.reduce_mean(tf.cast(pred_error, tf.float32))\n",
        "    return bber\n",
        "\n",
        "def random_sample(batch_size=32):\n",
        "    msg = np.random.randint(M, size=batch_size)\n",
        "    return msg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def B_Ber_m(input_msg, msg):\n",
        "    '''Calculate the Batch Bit Error Rate'''\n",
        "    pred_error = tf.not_equal(input_msg, tf.argmax(msg, 1))      \n",
        "    bber = tf.reduce_mean(tf.cast(pred_error, tf.float32))\n",
        "    return bber\n",
        "\n",
        "def SNR_to_noise(snrdb):\n",
        "    '''Transform EbNo[dB]/snr to noise power'''\n",
        "    snr = 10**(snrdb/10)\n",
        "    noise_std = 1/np.sqrt(2*snr)\n",
        "    return noise_std\n",
        "\n",
        "noise_std = EbNo2Sigma(SNR)\n",
        "\n",
        "print(EbNo2Sigma(SNR))\n",
        "print(EbNo_to_noise(SNR))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.15792649852735607\n",
            "0.15792649852735607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBGMgdDEh7uX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_encoding(M=16, n=1):\n",
        "    inp = np.arange(0,M)\n",
        "    coding = encoder.predict(inp)\n",
        "    fig = plt.figure(figsize=(4,4))\n",
        "    plt.plot(coding[:,0], coding[:, 1], \"b.\")\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
        "    plt.grid(True)\n",
        "    plt.gca().set_ylim(-2, 2)\n",
        "    plt.gca().set_xlim(-2, 2)\n",
        "    plt.show()\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOoYuK_jR9rH",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQxhmgOa0_7c",
        "colab_type": "text"
      },
      "source": [
        "#### Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXbS5lM9Tb9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_generator(n):\n",
        "  input1 = tf.keras.layers.Input(shape=(n,))\n",
        "  x1 = tf.keras.layers.Dense(n,kernel_initializer=randN_initial)(input1)\n",
        "  input2 =tf.random.normal(shape=tf.shape(input1))\n",
        "  #input2 =tf.random.normal([tf.shape(input1)[0],n]) r \n",
        "  x2 = tf.keras.layers.Dense(n,kernel_initializer=randN_initial)(input2)\n",
        "  subtracted = tf.keras.layers.Concatenate(1)([x1, x2])\n",
        "  h1 = tf.keras.layers.Dense(64,use_bias=True,  activation='relu')(subtracted)\n",
        "  h2 = tf.keras.layers.Dense(64,use_bias=True,kernel_initializer=ones_initial, activation='relu')(h1)\n",
        "  out = tf.keras.layers.Dense(n, use_bias= True, kernel_initializer=ones_initial,activation='linear')(h2)\n",
        "  generator = tf.keras.models.Model(inputs=[input1], outputs=out)\n",
        "  return generator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt2rTP7hSFt4",
        "colab_type": "text"
      },
      "source": [
        "#### Discriminator Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97h2eMLeXS68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_discriminator(n):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(32,use_bias=True, activation='relu',input_shape=((2*n,))))\n",
        "  model.add(tf.keras.layers.Dense(32,use_bias=True,  activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(1,use_bias=False, activation='sigmoid'))\n",
        "  return model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUkLPv0gvn-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = get_generator(n)\n",
        "discriminator=get_discriminator(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcIzLZj5Seh9",
        "colab_type": "text"
      },
      "source": [
        "## Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNHtzAC4SPBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gan_encoder(M):\n",
        "  model = keras.models.Sequential([\n",
        "            keras.layers.Embedding(M, M, embeddings_initializer='glorot_normal'),\n",
        "            keras.layers.Dense(M*2, activation=\"elu\"),\n",
        "            keras.layers.Dense(M*2, activation=\"elu\"),\n",
        "            keras.layers.Dense(n,kernel_initializer=ones_initial, activation=None),\n",
        "            e2,\n",
        "            EncOut,\n",
        "            GenIn])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5iCDE4dSL35",
        "colab_type": "text"
      },
      "source": [
        "## Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5KjEhDvSWQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gan_decoder(M):\n",
        "   model= keras.models.Sequential([\n",
        "                #DecIn,\n",
        "                #d1,\n",
        "                keras.layers.Input(shape=(n,)),\n",
        "                keras.layers.Dense(M*2, activation=\"elu\"),\n",
        "                keras.layers.Dense(M*2, activation=\"elu\"),\n",
        "                keras.layers.Dense(M, activation=\"softmax\")\n",
        "                ])\n",
        "   return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-t76sYBtlP1",
        "colab_type": "text"
      },
      "source": [
        "### GAN Training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-14T06:31:39.047233Z",
          "start_time": "2019-05-14T06:31:38.222179Z"
        },
        "colab_type": "code",
        "id": "dSYjNRAwSYb7",
        "colab": {}
      },
      "source": [
        "# optimizers\n",
        "w_gen_optimizer = tf.keras.optimizers.RMSprop(0.0001)# RMSprop(0.0001)#(0.0001, beta_1=0.5,beta_2 =0.1) # works as well\n",
        "w_disc_optimizer = tf.keras.optimizers.RMSprop(0.0001)#(0.0001, beta_1=0.5,beta_2 =0.1)  #(0.001)# train the model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "encoder = get_gan_encoder(M)\n",
        "decoder = get_gan_decoder(M)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4gYXZf9hkzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train(batch_size):\n",
        "  gen_gradients, disc_gradients = compute_gradients(batch_size)\n",
        "  apply_gradients(gen_gradients, disc_gradients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPsFbJwchvhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def compute_gradients(batch_size):\n",
        "\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    disc_loss, gen_loss = compute_loss(batch_size)\n",
        "    #tf.print(\"gen_loss:\", gen_loss)\n",
        "  disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "  gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  return gen_gradients, disc_gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-14T06:31:38.068468Z",
          "start_time": "2019-05-14T06:31:38.046751Z"
        },
        "colab_type": "code",
        "id": "Wyipg-4oSYb1",
        "colab": {}
      },
      "source": [
        "def compute_loss(batch_size):\n",
        "  \"\"\" passes through the network and computes loss\n",
        "  \"\"\"\n",
        "        ### pass through network\n",
        "        # generating noise from a uniform distribution\n",
        "  ####Mein noise ist anders als hier\n",
        "  gradient_penalty_weight = 0.1   #0.5\n",
        "  #print(x_samp)\n",
        "  # run noise through generator\n",
        "  m =random_sample(batch_size)\n",
        "  r = encoder(m)\n",
        "  real_data = tf.concat(values=[real_channel(r,noise_std), r], axis=1)\n",
        "  fake_data = tf.concat(values=[generator(r),r], axis=1)\n",
        "  # discriminate x and x_gen\n",
        "  logits_x = discriminator(real_data)\n",
        "  logits_x_gen = discriminator(fake_data)\n",
        "\n",
        "  # gradient penalty\n",
        "  d_regularizer = gradient_penalty(real_data, fake_data)\n",
        "        ### losses\n",
        "  disc_loss = (tf.reduce_mean(logits_x) - tf.reduce_mean(logits_x_gen)+ d_regularizer * gradient_penalty_weight)\n",
        "\n",
        "        # losses of fake with label \"1\"\n",
        "  gen_loss = tf.reduce_mean(logits_x_gen)\n",
        "  return disc_loss, gen_loss\n",
        "\n",
        "\n",
        "\n",
        "def apply_gradients(gen_gradients, disc_gradients):\n",
        "\n",
        "  w_gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
        "  w_disc_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
        "\n",
        "def gradient_penalty(x, x_gen):\n",
        "  epsilon = tf.random.uniform([x.shape[0], 1, 1, 1], 0.0, 1.0)\n",
        "  #epsilon = tf.random.uniform(shape = x.shape, minval= 0.0, maxval= 1.0)\n",
        "  x_hat = epsilon * x + (1 - epsilon) * x_gen\n",
        "  with tf.GradientTape() as t:\n",
        "      t.watch(x_hat)\n",
        "      d_hat = discriminator(x_hat)\n",
        "  gradients = t.gradient(d_hat, x_hat)\n",
        "  ddx = tf.sqrt(tf.reduce_sum(gradients ** 2, axis=[1, 2]))\n",
        "  d_regularizer = tf.reduce_mean((ddx - 1.0) ** 2)\n",
        "  #tf.print(\"gradient_penalty\")\n",
        "  #tf.print(d_regularizer)\n",
        "  return d_regularizer\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-14T06:31:39.056490Z",
          "start_time": "2019-05-14T06:31:39.049635Z"
        },
        "colab_type": "code",
        "id": "47sz8RMeSYb-",
        "colab": {}
      },
      "source": [
        "def generate_evaluation_data(batch_size=100):\n",
        "  x = tf.random.normal((batch_size,n),dtype=tf.dtypes.float32)    #randomly sample input data (\"fake\" AE messages)\n",
        "  x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) #Average power normalization (not required if standard normal distribution is used )\n",
        "  fake_eval_data = generator([x])\n",
        "  real_eval_data = real_channel(x,noise_std) #tf.concat(values=[real_channel(x),x], axis=1)\n",
        "  inputs = x\n",
        "  return  real_eval_data, fake_eval_data, inputs \n",
        "\n",
        "\n",
        "\n",
        "def get_evaluation_data(evaluation_per_epochs=100):\n",
        "  real_eval_data = []\n",
        "  fake_eval_data  = []\n",
        "  inputs = []\n",
        "  for i in range(evaluation_per_epochs):\n",
        "    data = generate_evaluation_data()\n",
        "    real_eval_data.append(data[0])\n",
        "    fake_eval_data.append(data[1])\n",
        "    inputs.append(data[2])\n",
        "  return real_eval_data, fake_eval_data, inputs\n",
        "\n",
        "\n",
        "def test_eval(real_eval_data,fake_eval_data,inputs):\n",
        "  hist_range = 1\n",
        "  \n",
        "  \n",
        "  fake_output_hist = np.mean(fake_eval_data,axis=0)  # Changed from 0 to 1\n",
        "  real_output_hist = np.mean(real_eval_data,axis=0)\n",
        "  inputs_hist = np.mean(inputs,axis=0)\n",
        "    \n",
        "  fake_output_hist1 = np.reshape( fake_output_hist,[-1,])\n",
        "  real_output_hist1 = np.reshape( real_output_hist,[-1,])\n",
        "    \n",
        "  plt.hist(fake_output_hist1,bins=100,range=(-hist_range,hist_range),density=True,histtype='step')\n",
        "  plt.hist(real_output_hist1,bins=100,range=(-hist_range,hist_range),density=True,histtype='step')    \n",
        "  plt.title(\"noise distribution\")\n",
        "  plt.legend([\"generator\", \"target\"])\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leKOzDNGflwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = pd.DataFrame(columns = ['disc_loss', 'gen_loss'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-14T07:04:26.791634Z",
          "start_time": "2019-05-14T07:04:17.126436Z"
        },
        "colab_type": "code",
        "id": "00dI2M4iSYcE",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "def gen_train(n_epochs,batch_size):\n",
        "  generator.trainable =True\n",
        "  start = time.time()\n",
        "  for epoch in range(n_epochs):\n",
        "    x= tf.random.normal((batch_size,n),dtype=tf.dtypes.float32)\n",
        "    x_samp =x/tf.sqrt(2*tf.reduce_mean(tf.square(x)))\n",
        "    train(batch_size)\n",
        "    # test on holdout\n",
        "    loss = []\n",
        "    if epoch%500 == 0:\n",
        "      real_c = tf.concat(values=[real_channel(x,noise_std), x], axis=1)\n",
        "      fake_c = generator(x)\n",
        "      real_eval_data, fake_eval_data, inputs = get_evaluation_data()\n",
        "      test_eval(real_eval_data, fake_eval_data, inputs)\n",
        "      tf.print(fake_c[0])\n",
        "      #tf.print(disc_loss, gen_loss)\n",
        "    real_data = tf.concat(values=[real_channel(x,noise_std), x], axis=1)  \n",
        "    loss.append(compute_loss(batch_size))\n",
        "    losses.loc[len(losses)] = np.mean(loss, axis=0)\n",
        "    if epoch%100 == 0:\n",
        "      print(\n",
        "         \"Epoch: {} | disc_loss: {} | gen_loss: {}\".format(\n",
        "              epoch, losses.disc_loss.values[-1], losses.gen_loss.values[-1]\n",
        "         )  )\n",
        "  tf.saved_model.save(generator,'/tmp/saved_model/')\n",
        "  plt.plot(losses.disc_loss.values)\n",
        "  generator.trainable =False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y82FQj3Jmvxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def creating_and_train_gan(epochs,n_steps, batch_size, SNR_level , n ):  #optional Leraning Rates\n",
        "  generator.trainable = True\n",
        "  train_gan(epochs, n_steps, batch_size, SNR_level)\n",
        "  #4 after GAN training\n",
        " # generator.trainable = False\n",
        "  #tf.print(generator.trainable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLTtFoV0IoPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gan_Test_AE(data):\n",
        "    '''Calculate Bit Error for varying SNRs'''\n",
        "    snr_range = np.linspace(0, 15, 31)\n",
        "    bber_vec = [None] * len(snr_range)\n",
        "        \n",
        "    for db in range(len(snr_range)):           \n",
        "        noise_std = EbNo_to_noise(snr_range[db])\n",
        "        code_word = encoder(data)\n",
        "        rcvd_word = code_word + tf.random.normal(tf.shape(code_word), mean=0.0, stddev=noise_std)\n",
        "        dcoded_msg = decoder(rcvd_word)\n",
        "        bber_vec[db] = B_Ber_m(data, dcoded_msg)\n",
        "        if (db % 6 == 0) & (db > 0):\n",
        "            print(f'Progress: {db} of {30} parts')\n",
        "\n",
        "    return (snr_range, bber_vec)\n",
        "\n",
        "def Test_AE_rayleigh(data):\n",
        "    '''Calculate Bit Error for varying SNRs'''\n",
        "    snr_range = np.linspace(0, 15, 31)\n",
        "    bber_vec = [None] * len(snr_range)\n",
        "        \n",
        "    for db in range(len(snr_range)):           \n",
        "        noise_std = EbNo_to_noise(snr_range[db])\n",
        "        code_word = encoder(data)\n",
        "        rcvd_word = rayleigh_channel(code_word, noise_std)\n",
        "        dcoded_msg = decoder(rcvd_word)\n",
        "        bber_vec[db] = B_Ber_m(data, dcoded_msg)\n",
        "        if (db % 6 == 0) & (db > 0):\n",
        "            print(f'Progress: {db} of {30} parts')\n",
        "\n",
        "    return (snr_range, bber_vec)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI0Tj40kzY6X",
        "colab_type": "text"
      },
      "source": [
        "# Resiver Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS0X_xB4LBXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = get_gan_decoder(M)\n",
        "encoder = get_gan_encoder(M)\n",
        "\n",
        "\n",
        "channel_layer = keras.layers.Lambda(lambda x: real_channel(x,noise_std))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6ZV07BiRAlJ",
        "colab_type": "text"
      },
      "source": [
        "# Decoder Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8-PiMpOTXqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = random_sample(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx0FiLdHQ2m-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "loss_object_a = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "decoder_optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def decoder_loss(decoder, x, y):\n",
        "  x_ = encoder(x)\n",
        "  y1= channel_layer(x_)\n",
        "  #y1 = generator(x_)\n",
        "  y_ = decoder(y1)\n",
        "  return loss_object(y_true=y, y_pred=y_)\n",
        "\n",
        "#def decoder_loss2(decoder, x, y):\n",
        "#  x_ = encoder(x)\n",
        "#  y1 = generator(x_)\n",
        "#  y_ = decoder(y1)\n",
        "#  return loss_object(y_true=y, y_pred=y_)\n",
        "\n",
        "@tf.function\n",
        "def decoder_grad(model, inputs, targets):\n",
        "  with tf.GradientTape() as decoder_tape:\n",
        "    loss_value = decoder_loss(model, inputs, targets)\n",
        "  return loss_value, decoder_tape.gradient(loss_value, model.trainable_variables)\n",
        "\n",
        "#@tf.function\n",
        "#def decoder_grad2(model, inputs, targets):\n",
        "#  with tf.GradientTape() as decoder_tape:\n",
        "#    loss_value = decoder_loss2(model, inputs, targets)\n",
        "#  return loss_value, decoder_tape.gradient(loss_value, model.trainable_variables)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJu-Y3eEQ4k3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Keep results for plotting\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "def decoder_training(num_epochs,batch_size):\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    data = random_sample(batch_size)\n",
        "    loss_value = decoder_step_training()\n",
        "    epoch_loss_avg(loss_value)  # Add current batch loss\n",
        "    # Compare predicted label to actual label\n",
        "    epoch_accuracy(data, decoder(channel_layer(encoder(data))))   \n",
        "    train_loss_results.append(epoch_loss_avg.result())\n",
        "    train_accuracy_results.append(epoch_accuracy.result())\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "      print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                      epoch_accuracy.result()))\n",
        "@tf.function                                                                \n",
        "def decoder_step_training():\n",
        "  loss_value, decoder_grads = decoder_grad(decoder, data, data)\n",
        "  decoder_optimizer.apply_gradients(zip(decoder_grads, decoder.trainable_variables))\n",
        "  return loss_value  \n",
        "\n",
        "#@tf.function                                                                \n",
        "#def decoder_step_training2():\n",
        "#  loss_value, decoder_grads = decoder_grad2(decoder, data, data)\n",
        "#  decoder_optimizer.apply_gradients(zip(decoder_grads, decoder.trainable_variables))\n",
        "#  return loss_value  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbAJL0v_sLUz",
        "colab_type": "text"
      },
      "source": [
        "# Transmitter Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_u4AHItRcsT1",
        "colab": {}
      },
      "source": [
        "encoder_optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
        "\n",
        "def encoder_loss(encoder, x, y):\n",
        "  x_ = encoder(x)\n",
        "  y1= generator(x_)\n",
        "  y_ = decoder(y1)\n",
        "  return loss_object_a(y_true=y, y_pred=y_)\n",
        "\n",
        "@tf.function\n",
        "def encoder_grad(model, inputs, targets):\n",
        "  with tf.GradientTape() as encoder_tape:\n",
        "    loss_value = encoder_loss(model, inputs, targets)\n",
        "  return loss_value, encoder_tape.gradient(loss_value, model.trainable_variables)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUxy6g9kEakF",
        "colab_type": "code",
        "outputId": "b5efb654-79ee-47c4-be8b-d76260563772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "loss_value, encoder_grads = encoder_grad(encoder,data, data)\n",
        "\n",
        "print(\"Step: {}, Initial Loss: {}\".format(encoder_optimizer.iterations.numpy(),\n",
        "                                          loss_value.numpy()))\n",
        "\n",
        "encoder_optimizer.apply_gradients(zip(encoder_grads, encoder.trainable_variables))\n",
        "\n",
        "print(\"Step: {},         Loss: {}\".format(encoder_optimizer.iterations.numpy(),\n",
        "                                          encoder_loss(encoder, data, data).numpy()))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0, Initial Loss: 2.772629737854004\n",
            "Step: 1,         Loss: 2.772629499435425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8buZRqXNdcZz",
        "colab": {}
      },
      "source": [
        "\n",
        "# Keep results for plotting\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "def encoder_training(num_epochs, batch_size = 400):\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    data = random_sample(batch_size)\n",
        "\n",
        "    loss_value = encoder_step_training()\n",
        "      #loss_value, encoder_grads = encoder_grad(encoder, data, data)\n",
        "      #encoder_optimizer.apply_gradients(zip(encoder_grads, encoder.trainable_variables))\n",
        "    epoch_loss_avg(loss_value)  # Add current batch loss\n",
        "    # Compare predicted label to actual label\n",
        "    epoch_accuracy(data, decoder(generator(encoder(data))))  \n",
        "   # train_loss_results.append(epoch_loss_avg.result())\n",
        "   # train_accuracy_results.append(epoch_accuracy.result())\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "      print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                epoch_accuracy.result()))\n",
        "@tf.function\n",
        "def encoder_step_training():\n",
        "  loss_value, encoder_grads = encoder_grad(encoder, data, data)\n",
        "  encoder_optimizer.apply_gradients(zip(encoder_grads, encoder.trainable_variables))\n",
        "  return loss_value\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m0fQ6OXgPf1",
        "colab_type": "text"
      },
      "source": [
        "# GAN Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNFwwZrcgOkn",
        "colab_type": "code",
        "outputId": "c5919d2c-f286-43d3-c6e0-ae38f6678c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "%%time\n",
        "start = time.time()\n",
        "\n",
        "for i in range(3):\n",
        "  test_encoding(M,n)\n",
        "  decoder_training(501, 100)\n",
        "  gen_train(1001, 100)\n",
        "  encoder_training(501,100)\n",
        "  test_encoding(M,n)\n",
        "  decoder_training(1001, 100)\n",
        "  test_encoding(M,n)\n",
        "\n",
        "\n",
        "time_to_train_gan = time.time()-start\n",
        "tf.print ('Time for the training is {} sec,'.format( time.time()-start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASMAAAEWCAYAAAAtl/EzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndL8unQ8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 000: Loss: 2.768, Accuracy: 15.000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUbnLX0y_p6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "generator.trainable =False\n",
        "encoder.trainable = True\n",
        "decoder.trainable = True\n",
        "gan_AE = tf.keras.models.Sequential([encoder,generator,decoder])\n",
        "data = random_sample(10000000)\n",
        "start = time.time()\n",
        "gan_AE.compile(optimizer=keras.optimizers.Adam(0.0001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "history = gan_AE.fit(data, data, batch_size=500,steps_per_epoch=400, epochs=10)\n",
        "\n",
        "test_encoding(M,n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GW2opX7SwMo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# AE training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukO76l6yIoPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test msg sequence for normal encoding\n",
        "N_test = 500000\n",
        "test_msg = np.random.randint(M, size=N_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK5wA-zzHScv",
        "colab_type": "text"
      },
      "source": [
        "### Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M-S0sbhIoPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan_bber_data = gan_Test_AE(test_msg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYdEm0eQIoP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Approximate 16 QAM Error\n",
        "def SIXT_QAM_sim(ebno):\n",
        "    return (3.0/2)*special.erfc(np.sqrt((4.0/10)*10.**(ebno/10)))\n",
        "\n",
        "ebnodbs = np.linspace(0,15,16)\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "plt.semilogy(gan_bber_data[0], gan_bber_data[1], '^-')\n",
        "plt.semilogy(ebnodbs, SIXT_QAM_sim(ebnodbs), '*-');\n",
        "plt.gca().set_ylim(1e-5, 1)\n",
        "plt.gca().set_xlim(0, 15)\n",
        "plt.ylabel(\"Batch Symbol Error Rate\", fontsize=14, rotation=90)\n",
        "plt.xlabel(\"SNR [dB]\", fontsize=18)\n",
        "plt.legend(['AE with GAN', '16QAM'],\n",
        "           prop={'size': 14}, loc='upper right');\n",
        "plt.grid(True, which=\"both\")\n",
        "#print('time to train the AE Model with GAN',time_to_train_gan)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBes21qLlcS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bber_data_rayleigh = Test_AE_rayleigh(test_msg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q60pg9b9Rz7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SIXT_QAM_sim(ebno):\n",
        "    return (3.0/2)*special.erfc(np.sqrt((4.0/10)*10.**(ebno/10)))\n",
        "\n",
        "ebnodbs = np.linspace(0,15,16)\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "plt.semilogy(gan_bber_data[0], gan_bber_data[1], 'o-')\n",
        "plt.semilogy(bber_data_rayleigh[0], bber_data_rayleigh[1], '+-')\n",
        "#plt.semilogy(ebnodbs, SIXT_QAM_sim(ebnodbs), '^-');\n",
        "plt.gca().set_ylim(1e-5, 1)\n",
        "plt.gca().set_xlim(0, 15)\n",
        "plt.ylabel(\"Batch Symbol Error Rate\", fontsize=14, rotation=90)\n",
        "plt.xlabel(\"SNR [dB]\", fontsize=18)\n",
        "plt.legend(['BLER for Gauss','BLER for Rayleigh'],\n",
        "           prop={'size': 14}, loc='upper right');\n",
        "plt.grid(True, which=\"both\")\n",
        "#plt.savefig('home/ben/Downloads/MineRayleigh.eps', format='eps')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR1kh-fxcASt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}